{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc2d24b",
   "metadata": {},
   "source": [
    "## Script Plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a521c5",
   "metadata": {},
   "source": [
    "1. Imports\n",
    "2. Define Training Parameters + Variables:\n",
    "    - Epochs\n",
    "    - Batch_size\n",
    "    - Random Seed\n",
    "    - Input Size\n",
    "    - Patience\n",
    "    - Learning Rate\n",
    "    - Input Data Folder - Labelled Folders\n",
    "3. Data Import/Organisation\n",
    "    - TF import data (from folder)\n",
    "    - Train/Val/Test Splitting\n",
    "    - K-Fold Validation\n",
    "4. Load and Prep Models\n",
    "    - Pretrained ENB3 (Freezing AFIB Networks, Retraining Classifier)\n",
    "    - Pretrained ENB3 (Unfrozen)\n",
    "    - Fresh ENB3 (Frozen ImageNet Pretraining)\n",
    "    - Fresh ENB3 (Unfrozen ImageNet Pretraining)\n",
    "5. Training Iterations\n",
    "    - Binary (0+1R VS 2R)\n",
    "    - Binary (0 vs 1+2R)\n",
    "    - Multiclass (0R VS 1R Vs 2R)\n",
    "    - Tran/Val/test (60%/20%/20%) vs 5-Folds Cross-validation \n",
    "6. Validation/Test Results\n",
    "    - Write Results Function\n",
    "\n",
    "Training plan: \n",
    "1. SWOIP: single with only image pretraining \n",
    "\n",
    "Tuning: \n",
    "1. Learning rate \n",
    "2. Patience \n",
    "3. Freezing to unfrozen certain amounts of layers \n",
    "4. change binary classifications \n",
    "5. multiclass \n",
    "6. ensembling 4 ENB3 models, feed each model output into 10 classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cd864",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0989e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay, roc_auc_score\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import csv\n",
    "\n",
    "from keras import regularizers\n",
    "import time \n",
    "import itertools\n",
    "\n",
    "print (tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340789a4",
   "metadata": {},
   "source": [
    "## TRAINING PARAMETERS & VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Parameters ###\n",
    "seed = 2806\n",
    "batch_size = 8 #16 \n",
    "# learning_rate = 0.0001\n",
    "max_epochs = 100\n",
    "# patience = 15\n",
    "image_size = 300\n",
    "img_width, img_height = image_size, image_size\n",
    "\n",
    "### Label Information ### \n",
    "# target_labels = ['mild rejection','moderate-to-severe rejection','no rejection']\n",
    "# classifier = 'binary'\n",
    "training_style = \"Ensemble\"\n",
    "Dataset = \"Binary-1\"\n",
    "Pretrained_model = \"MI\"\n",
    "\n",
    "# ==== Create target directory for saving/loading models === \n",
    "project_dir = 'D://Rui//'\n",
    "\n",
    "# === Create net directory for saving/loading models === \n",
    "date = datetime.now()\n",
    "net_dir = project_dir + 'Transplant_prediction_models//' + training_style + Pretrained_model + '_' + Dataset + '_' + str(date.day).rjust(2, '0')+str(date.month).rjust(2, '0')+str(date.year)+'_'+ str(date.hour)+str(date.minute)\n",
    "os.mkdir(net_dir)\n",
    "\n",
    "if Dataset == \"Binary-1\":\n",
    "    target_labels = ['0-1R', \"2R\"]\n",
    "    nClasses = 2\n",
    "elif Dataset == \"Binary-2\":\n",
    "    target_labels = ['0', \"1-2R\"]\n",
    "    nClasses = 2\n",
    "\n",
    "# === Pre-trained Net Location ===\n",
    "if Pretrained_model == \"AF\":\n",
    "    pretrained_net_dir = project_dir + 'AFIBvsNOTNet(080120254x3cols)//'\n",
    "elif Pretrained_model == \"MI\":\n",
    "    pretrained_net_dir = project_dir + 'MulticlassMIPTBXLTraining(4x3)//'\n",
    "\n",
    "print(\"target_labels =\", target_labels)\n",
    "print(\"nClasses = \", nClasses)\n",
    "# print(pretrained_net_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2b19c",
   "metadata": {},
   "source": [
    "## DATA IMPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0593e54",
   "metadata": {},
   "source": [
    "### TFDS Import & Organise Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfe928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input Folders\n",
    "if Dataset == \"Binary-1\":\n",
    "    dataset_dir = \"D://Rui//NIDACT2 Transplant ECGs//Multiclass_HTx_Rejection_Dataset_1(300_300)//Binary1_0-1Rvs2R\"\n",
    "elif Dataset == \"Binary-2\": \n",
    "    dataset_dir = \"D://Rui//NIDACT2 Transplant ECGs//Multiclass_HTx_Rejection_Dataset_1(300_300)//Binary2_0vs1-2R\"\n",
    "\n",
    "print(\"dataset_dir =\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cb6dc",
   "metadata": {},
   "source": [
    "### Train/val/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = dataset_dir + '(Splits)//train//'\n",
    "# train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory = train_dir,\n",
    "#     # labels = \"inferred\"\n",
    "#     label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "#     class_names = target_labels, \n",
    "#     # colour_mode = \"rgb\" -> three colour channels image\n",
    "#     batch_size = batch_size,\n",
    "#     image_size = (img_height, img_width),\n",
    "#     shuffle = True, \n",
    "#     seed = seed\n",
    "#     # validation_split = 0.2, # Spliting dataset into 80% training, 20% validation set\n",
    "#     # subset = \"both\", \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dir = dataset_dir + '(Splits)//val//'\n",
    "# val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory = val_dir,\n",
    "#     # labels = \"inferred\"\n",
    "#     label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "#     class_names = target_labels, \n",
    "#     # colour_mode = \"rgb\" -> three colour channels image\n",
    "#     batch_size = batch_size,\n",
    "#     image_size = (img_height, img_width),\n",
    "#     shuffle = True, \n",
    "#     seed = seed,\n",
    "#     # validation_split = 0.2, # Spliting dataset into 80% training, 20% validation set\n",
    "#     # subset = \"both\", \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34757272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir = dataset_dir + '(Splits)//test//'\n",
    "# test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory = test_dir,\n",
    "#     # labels = \"inferred\"\n",
    "#     label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "#     class_names = target_labels, \n",
    "#     # colour_mode = \"rgb\" -> three colour channels image\n",
    "#     batch_size = batch_size,\n",
    "#     image_size = (img_height, img_width),\n",
    "#     shuffle = True, \n",
    "#     seed = seed\n",
    "#     # validation_split = 0.2, # Spliting dataset into 80% training, 20% validation set\n",
    "#     # subset = \"both\", \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a2d96",
   "metadata": {},
   "source": [
    "### Sixty Forty Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = dataset_dir + '(6040)//train//'\n",
    "# train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory = train_dir,\n",
    "#     # labels = \"inferred\"\n",
    "#     label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "#     class_names = target_labels, \n",
    "#     # colour_mode = \"rgb\" -> three colour channels image\n",
    "#     batch_size = batch_size,\n",
    "#     image_size = (img_height, img_width),\n",
    "#     shuffle = True, \n",
    "#     seed = seed\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dir = dataset_dir + '(6040)//val//'\n",
    "# val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory = val_dir,\n",
    "#     # labels = \"inferred\"\n",
    "#     label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "#     class_names = target_labels, \n",
    "#     # colour_mode = \"rgb\" -> three colour channels image\n",
    "#     batch_size = batch_size,\n",
    "#     image_size = (img_height, img_width),\n",
    "#     shuffle = True, \n",
    "#     seed = seed\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = dataset_dir,\n",
    "    # labels = \"inferred\"\n",
    "    label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "    class_names = target_labels, \n",
    "    # colour_mode = \"rgb\" -> three colour channels image\n",
    "    batch_size = batch_size,\n",
    "    image_size = (img_height, img_width),\n",
    "    shuffle = True, \n",
    "    seed = seed,\n",
    "    validation_split = 0.4, # Spliting dataset into 60% training, 40% validation set\n",
    "    subset = \"both\", \n",
    ")\n",
    "\n",
    "train_ds = both_ds[0]\n",
    "val_ds = both_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88437c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== George ==== #\n",
    "# train_temp_labels = np.empty([0,2])\n",
    "\n",
    "# for batch,label in train_ds:\n",
    "#         train_temp_labels = np.concatenate([train_temp_labels, label], axis=0)\n",
    "\n",
    "# # print(train_temp_labels)\n",
    "# print(len(train_temp_labels))\n",
    "# print(np.sum(train_temp_labels,axis=0))\n",
    "\n",
    "# val_temp_labels = np.empty([0,2])\n",
    "\n",
    "# for batch, label in val_ds:\n",
    "#         val_temp_labels = np.concatenate([val_temp_labels, label], axis=0)\n",
    "\n",
    "# # print(val_temp_labels)\n",
    "# print(len(val_temp_labels))\n",
    "# print(np.sum(val_temp_labels,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c312f",
   "metadata": {},
   "source": [
    "## LOAD MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9d0b8",
   "metadata": {},
   "source": [
    "### Load fresh ENB3 Function (SWOIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENB3_model = keras.applications.EfficientNetB3(\n",
    "#     include_top = True # whether to include the fully-connected layer at the top of the network \n",
    "#     # weights = \"imagenet\" / \"none\" -> never seen imageNet \n",
    "#     # input_tensor = None\n",
    "#     # input_shape = None \n",
    "#     # pooling = None\n",
    "#     # classes = 1000\n",
    "#     # classifier_activation = \"softmax\"\n",
    "# )\n",
    "# # Freezing \n",
    "# for layer in ENB3_model.layers[:385]: #frozen to layer 125, unfreezed the rest \n",
    "#     layer.trainable = True # false: freezing, true: unfreeze \n",
    "# # How to look into the structure of the model \n",
    "# # loaded_model.layers[385]\n",
    "# # summary = loaded_model.summary()\n",
    "# # print(\"model summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b381d",
   "metadata": {},
   "source": [
    "### Load four Pretrained ENB3 Function (AF OR MI model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [125, 198, 272, 385]\n",
    "pretrained_ensemble_models = []\n",
    "for name in model_names: \n",
    "    model = tf.keras.models.load_model(\n",
    "            filepath = pretrained_net_dir + \"//\" + str(name) + \".h5\"\n",
    "            # custom_objects=None, \n",
    "            # compile=True, \n",
    "            # safe_mode=True\n",
    "            )\n",
    "    for layer in model.layers[:name]:\n",
    "        layer.trainable = True # false: freezing, true: unfreeze \n",
    "    pretrained_ensemble_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a4680",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efb778",
   "metadata": {},
   "source": [
    "### Adding classifier layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS ONE script \n",
    "def add_classification_layers(cutModel): #l1_reg, l2_reg, dropout_rate_1, dropout_rate_2, nClasses\n",
    "    classifier = cutModel.output\n",
    "    classifier = tf.keras.layers.GlobalAveragePooling2D()(classifier) #AveragePooling2D(pool_size=(4,4)) GlobalAveragePooling2D()\n",
    "    # First dense layer with customizable regularization and dropout\n",
    "    classifier = tf.keras.layers.Dense(128, activation=\"relu\", \n",
    "                                     kernel_regularizer=regularizers.L1L2(l1=0.001, l2=0.001))(classifier) #l2_reg\n",
    "    classifier = tf.keras.layers.Dropout(0.4)(classifier) #dropout_rate_1\n",
    "    # Second dense layer with customizable regularization and dropout\n",
    "    classifier = tf.keras.layers.Dense(64, activation=\"relu\", \n",
    "                                     kernel_regularizer=regularizers.L1L2(l1=0.0001, l2=0.0001))(classifier)\n",
    "    classifier = tf.keras.layers.Dropout(0.5)(classifier) #dropout_rate_2\n",
    "    classifier = tf.keras.layers.Dense(nClasses, activation=\"softmax\")(classifier)\n",
    "    full_model = tf.keras.Model(inputs=cutModel.input, outputs=classifier)\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45fc26",
   "metadata": {},
   "source": [
    "### Training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de08471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS ONE script \n",
    "def training_function(full_model, filepath, train_ds, val_ds): \n",
    "    # Define optimizer with an initial learning rate \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001) # This is my initial learning rate \n",
    "\n",
    "    # Define ReduceLROnPlateau callback (based on validation loss)\n",
    "    reduce_lr_patience = 10\n",
    "    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2, # Reduce LR by a factor of 5 (1/5 = 0.2)\n",
    "        patience=reduce_lr_patience,\n",
    "        min_lr=0.00001, # Don't let the LR get ridiculously small\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Early stopping \n",
    "    early_stopping_patience = 25 #50\n",
    "    early_stopper = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=early_stopping_patience,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True # Automatically restore the model from the best epoch\n",
    "    )\n",
    "\n",
    "    saving_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, \n",
    "        monitor=\"val_loss\", \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=False, \n",
    "        save_freq=\"epoch\",\n",
    "        initial_value_threshold=None\n",
    "    )\n",
    "\n",
    "    full_model.compile(optimizer=optimizer, \n",
    "                    loss = tf.keras.losses.BinaryCrossentropy(), #CategoricalFocalCrossentropy\n",
    "                    metrics=['binary_accuracy'])\n",
    "    \n",
    "    # Calculate class weights \n",
    "    labels_train = []\n",
    "    for _ , label in train_ds:\n",
    "        labels_train.append (np.array (label))\n",
    "    labels_train = np.concatenate(labels_train)\n",
    "    weight_function = lambda i: (1/sum(labels_train[:,i])*(labels_train.shape[0]/labels_train.shape[1]))\n",
    "    class_weights = {i: weight_function(i) for i in range(labels_train.shape[1])}\n",
    "    # print(class_weights)\n",
    "\n",
    "    # Training \n",
    "    results = full_model.fit(train_ds,\n",
    "                        batch_size=batch_size, \n",
    "                        validation_data=val_ds, \n",
    "                        epochs=max_epochs,\n",
    "                        callbacks=[early_stopper, lr_callback, saving_callback], # early_stopper, lr_callback\n",
    "                        class_weight = class_weights\n",
    "                        )\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf3f89",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss_function(results, filepath): \n",
    "    # Extract validation loss for easier access\n",
    "    val_loss = results.history['val_loss']\n",
    "\n",
    "    ### 1. Find the Best Epoch and Minimum Validation Loss\n",
    "    # Use np.argmin to find the INDEX of the minimum loss\n",
    "    best_epoch_idx = np.argmin(val_loss)\n",
    "    min_val_loss = val_loss[best_epoch_idx]\n",
    "\n",
    "    print(f\"Minimum validation loss of {min_val_loss:.4f} was found at epoch {best_epoch_idx}.\")\n",
    "\n",
    "    # ### 2. Calculate a Suggested Patience ###\n",
    "    # # Define how much worse the loss can get before we consider it overfitting\n",
    "    # tolerance_percentage = 0.05 # e.g., 5% worse than the minimum\n",
    "    # overfit_threshold = min_val_loss * (1 + tolerance_percentage)\n",
    "\n",
    "    # # Find the first epoch *after* the best epoch where loss exceeds the threshold\n",
    "    # patience_epoch = -1\n",
    "    # for i in range(best_epoch_idx + 1, len(val_loss)):\n",
    "    #     if val_loss[i] > overfit_threshold:\n",
    "    #         patience_epoch = i\n",
    "    #         break # Stop as soon as we find it\n",
    "\n",
    "    # # The suggested patience is the number of epochs between the best and the overfit point\n",
    "    # suggested_patience = -1\n",
    "    # if patience_epoch != -1:\n",
    "    #     suggested_patience = patience_epoch - best_epoch_idx\n",
    "\n",
    "    ### 3. Plot the Results and Annotations ###\n",
    "    plt.rcParams.update({'font.family':'sans-serif', 'font.sans-serif':['Arial']})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results.history['loss'], label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "\n",
    "    # # Annotate the minimum validation loss\n",
    "    # plt.axvline(x = best_epoch_idx, color = 'r', linestyle='--', label = f'Best Epoch: {best_epoch_idx}')\n",
    "    # plt.scatter(best_epoch_idx, min_val_loss, color = 'red', zorder = 5) # Mark the best point\n",
    "\n",
    "    # # Annotate the suggested patience point, if found\n",
    "    # if suggested_patience > 0:\n",
    "    #     plt.axvline(x=patience_epoch, color='g', linestyle='--', label=f'Recommended patience: {suggested_patience}')\n",
    "    #     plt.scatter(patience_epoch, val_loss[patience_epoch], color='green', zorder=5)\n",
    "    #     print(f\"A good starting patience value could be around {suggested_patience}.\")\n",
    "    #     print(f\"This is how many epochs it took for the validation loss to increase by more than {tolerance_percentage:.0%} from its minimum (at {patience_epoch} epoch).\")\n",
    "    # else:\n",
    "    #     print(\"\\nThe model did not appear to overfit within the training run (based on the defined tolerance).\")\n",
    "\n",
    "    plt.title('Training and Validation Loss Analysis')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (Binary Cross Entropy)') \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.savefig(filepath + 'loss graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_all_losses(histories, split_layers, filepath):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.rcParams.update({'font.family':'sans-serif', 'font.sans-serif':['Arial']})\n",
    "    # Map each layer number to a specific color\n",
    "    color_map = {\n",
    "        125: 'red',\n",
    "        198: 'blue',\n",
    "        272: 'green',\n",
    "        385: 'purple'\n",
    "    }\n",
    "\n",
    "    # Loop through each history object and its corresponding split layer\n",
    "    for history, layer in zip(histories, split_layers):\n",
    "        color = color_map.get(layer, 'grey') # Default to grey if layer not in map\n",
    "        label_prefix = f'truncated @{layer}'\n",
    "\n",
    "        # Plot VALIDATION loss with a SOLID line\n",
    "        plt.plot(\n",
    "            history.history['val_loss'],\n",
    "            color=color,\n",
    "            linestyle='-',  # Solid line\n",
    "            label=f'Val loss for model {label_prefix}'\n",
    "        )\n",
    "\n",
    "        # Plot TRAINING loss with a DASHED line\n",
    "        plt.plot(\n",
    "            history.history['loss'],\n",
    "            color=color,\n",
    "            linestyle='--', # Dashed line\n",
    "            label=f'Train loss for model {label_prefix}'\n",
    "        )\n",
    "    \n",
    "    plt.title('Combined Training & Validation Loss for 4 Truncated ENB3 models')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (Binary Cross Entropy)')\n",
    "    plt.legend()\n",
    "    # plt.grid(color='grey', linestyle='--', linewidth=0.5)\n",
    "    plt.savefig(filepath + '_combined_loss_graphs.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f65123",
   "metadata": {},
   "source": [
    "### Training (SWOIP/SWAFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a8e84",
   "metadata": {},
   "source": [
    "#### GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea02d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ======== GRID SEARCH (SWAFP) ============= ###\n",
    "# # Define the grid of hyperparameters to search\n",
    "# hyperparameter_grid = {\n",
    "#     'l1_reg': [0.0001, 0.001, 0.01],\n",
    "#     'l2_reg': [0.0001, 0.001, 0.01],\n",
    "#     'dropout_rate_1': [0.3, 0.4, 0.5],\n",
    "#     'dropout_rate_2': [0.3, 0.4, 0.5]\n",
    "# }\n",
    "\n",
    "# # This is the main block for SWAFP\n",
    "# if training_style == \"SWAFP\":\n",
    "#     split_layers = [125, 198, 272, 385]\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_hyperparameters = None\n",
    "\n",
    "#     keys = hyperparameter_grid.keys()\n",
    "#     values = hyperparameter_grid.values()\n",
    "\n",
    "#     # The outer grid search loop\n",
    "#     for combo in itertools.product(*values):\n",
    "#         params = dict(zip(keys, combo))\n",
    "#         print(f\"--- Training with hyperparameters: {params} ---\")\n",
    "\n",
    "#         all_results = []\n",
    "#         pretrained_ensemble_models_copy = list(pretrained_ensemble_models) # Create a copy to prevent overwriting\n",
    "\n",
    "#         # This is the inner training loop for the ensemble\n",
    "#         for idx, split_layer in enumerate(split_layers):\n",
    "#             loaded_model = pretrained_ensemble_models_copy[idx]\n",
    "#             cutModel = tf.keras.Model(loaded_model.input, loaded_model.layers[split_layer-1].output)\n",
    "\n",
    "#             # Build the model with the current hyperparameters\n",
    "#             full_model = add_classification_layers(\n",
    "#                 cutModel,\n",
    "#                 nClasses=nClasses,\n",
    "#                 l1_reg=params['l1_reg'],\n",
    "#                 l2_reg=params['l2_reg'],\n",
    "#                 dropout_rate_1=params['dropout_rate_1'],\n",
    "#                 dropout_rate_2=params['dropout_rate_2']\n",
    "#             )\n",
    "\n",
    "#             # Define a unique filepath for each hyperparameter combination and model\n",
    "#             filepath = os.path.join(net_dir, f\"hparams__{str(combo)}__model_{split_layer}\")\n",
    "#             modelpath = filepath + \"_\" + \"model.keras\"\n",
    "\n",
    "#             print(f\"--- Training model truncated at layer {split_layer} --- \")\n",
    "\n",
    "#             results = training_function(full_model, modelpath, train_ds, val_ds)\n",
    "#             all_results.append(results)\n",
    "\n",
    "#         # Evaluate the performance of this hyperparameter combination\n",
    "#         # For simplicity, let's take the average of all ensemble models' best validation loss\n",
    "#         avg_min_val_loss = np.mean([min(res.history['val_loss']) for res in all_results])\n",
    "\n",
    "#         # Check if this combination is the best so far\n",
    "#         if avg_min_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_min_val_loss\n",
    "#             best_hyperparameters = params\n",
    "#             print(f\"New best hyperparameter combination found! Average Val Loss: {best_val_loss:.4f} with params: {best_hyperparameters}\")\n",
    "\n",
    "#     print(\"--- Grid search complete ---\")\n",
    "#     print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "#     print(f\"Best average validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52465a31",
   "metadata": {},
   "source": [
    "#### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ENSEMBLE MODELS Function \n",
    "def train_ensemble_models(base_models, split_layers, net_dir, training_style, train_ds, val_ds):\n",
    "    results = []\n",
    "    print(\"--- Starting Ensemble Training ---\")\n",
    "\n",
    "    # Zip combines the two lists, to get one model and one split_layer per iteration.\n",
    "    for base_model, split_layer in zip(base_models, split_layers):\n",
    "        print(f\"--- Training model truncated at layer {split_layer} --- \")\n",
    "        \n",
    "        # 1. Truncate the base model\n",
    "        cut_model = tf.keras.Model(inputs=base_model.input, outputs=base_model.layers[split_layer - 1].output)\n",
    "        \n",
    "        # 2. Add new classification layers\n",
    "        full_model = add_classification_layers(cut_model)\n",
    "        \n",
    "        # 3. Define paths and train the model\n",
    "        filepath = os.path.join(net_dir, f\"{training_style}_{split_layer}\")\n",
    "        modelpath = f\"{filepath}_model.keras\"\n",
    "        \n",
    "        t_start_model = time.time()\n",
    "        history = training_function(full_model, modelpath, train_ds, val_ds)\n",
    "        results.append(history)\n",
    "        t_end_model = time.time()\n",
    "        print(f\"--- Model training finished in {t_end_model - t_start_model:.2f} seconds ---\")\n",
    "\n",
    "    print(\"--- All training complete. Generating combined graph. ---\")\n",
    "    combined_filepath = os.path.join(net_dir, training_style)\n",
    "    \n",
    "    # The graphing function with the collected results\n",
    "    graph_all_losses(results, split_layers, combined_filepath)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "split_layers = [125, 198, 272, 385]\n",
    "\n",
    "# Determine the list of base models to use\n",
    "if Pretrained_model == \"ENB3\":\n",
    "    models_to_train = [ENB3_model] * len(split_layers)\n",
    "else:\n",
    "    models_to_train = pretrained_ensemble_models\n",
    "\n",
    "# Call the training function\n",
    "training_histories = train_ensemble_models(\n",
    "    base_models=models_to_train,\n",
    "    split_layers=split_layers,\n",
    "    net_dir=net_dir,\n",
    "    training_style=training_style,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4cc09",
   "metadata": {},
   "source": [
    "### Generate a smoothed loss curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a34804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_series = pd.Series(val_loss)\n",
    "\n",
    "# # Calculate a moving average over a window of 20 epochs\n",
    "# smoothed_val_loss = val_loss_series.rolling(window=20, center=True).mean()\n",
    "\n",
    "# # Plot both for comparison\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(val_loss, label='Raw Validation Loss', color = 'green', alpha = 0.4)\n",
    "# plt.plot(smoothed_val_loss, label='Smoothed Validation Loss (20-Epoch Avg)', color='orange', linewidth=2)\n",
    "\n",
    "# plt.title('Smoothed Loss Curve')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss (Categorical Cross Entropy)') \n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1c09a",
   "metadata": {},
   "source": [
    "## MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9395a",
   "metadata": {},
   "source": [
    "### Write Results File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eafc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResultsFile(labels_test,ensemble_prediction,ensemble_scores,nClasses, t_start,t_end,writeDir):\n",
    "    labelResults = []\n",
    "    # Binary Classifier\n",
    "    if nClasses == 2:\n",
    "        positive_labels = sum(labels_test)\n",
    "        negative_labels = len(labels_test) - positive_labels\n",
    "        tn, fp, fn, tp = confusion_matrix(labels_test,ensemble_prediction).ravel()\n",
    "        accuracy = (accuracy_score(labels_test,ensemble_prediction))\n",
    "        precision = (precision_score(labels_test,ensemble_prediction))\n",
    "        recall = (recall_score(labels_test,ensemble_prediction))\n",
    "        f1 = (f1_score(labels_test,ensemble_prediction))\n",
    "        train_time = (t_end-t_start)\n",
    "        fpr, tpr, _ = roc_curve(labels_test, ensemble_scores[:,1])\n",
    "        roc_auc = roc_auc_score(labels_test, ensemble_scores[:,1])\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "        roc_display.figure_.savefig(writeDir + '/ROC.png')\n",
    "        \n",
    "        header = ['Positive Labels', 'Negative Labels', 'TP', 'FP', 'TN', 'FN', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Training Time']\n",
    "        with open(writeDir + '/' + 'Ensemble results.csv', 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            writer.writerow([positive_labels, negative_labels, tp, fp, tn, fn, accuracy, precision, recall, f1, roc_auc, train_time])\n",
    "        \n",
    "    # Multiclass\n",
    "    elif nClasses > 2:\n",
    "        \n",
    "        binary_truths = labels_test\n",
    "        binary_preds = np.round(ensemble_scores) # using rounding to calculate confusion matrix\n",
    "        label_names = target_labels\n",
    "            \n",
    "        for labelID in range(nClasses):\n",
    "            name = label_names[labelID]\n",
    "            y_truth = binary_truths[:,labelID]\n",
    "            y_pred = binary_preds[:,labelID]\n",
    "            positive_labels = sum(y_truth)\n",
    "            negative_labels = len(y_truth) - positive_labels\n",
    "            tn, fp, fn, tp  = confusion_matrix(y_truth,y_pred).ravel()\n",
    "            accuracy = (accuracy_score(y_truth,y_pred))\n",
    "            precision = (precision_score(y_truth,y_pred))\n",
    "            recall = (recall_score(y_truth,y_pred))\n",
    "            f1 = (f1_score(y_truth,y_pred))\n",
    "            train_time = (t_end-t_start)\n",
    "            fpr, tpr, _ = roc_curve(binary_truths[:,labelID], ensemble_scores[:,labelID])\n",
    "            roc_auc = roc_auc_score(binary_truths[:,labelID], ensemble_scores[:,labelID])\n",
    "            roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "            roc_display.figure_.savefig(writeDir + '/ROC ' + name + '.png')\n",
    "            labelResults.append([name, positive_labels, negative_labels, tp, fp, tn, fn, accuracy, precision, recall, f1, roc_auc, train_time])\n",
    "            \n",
    "            header = ['Label', 'Positive Labels', 'Negative Labels', 'TP', 'FP', 'TN', 'FN', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Training Time']\n",
    "\n",
    "        with open(writeDir + '/' + ' Ensemble results.csv', 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(labelResults)\n",
    "\n",
    "    # for idx, matrix in enumerate(matrices):\n",
    "    #     disp = ConfusionMatrixDisplay(matrix)\n",
    "    #     disp.plot()\n",
    "    #     plt.show()\n",
    "    #     plt.savefig([writeDir + '\\\\' + label_code[idx] + ' Confusion Matrix.png'])\n",
    "    #     plt.close()\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return labelResults, header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8db534",
   "metadata": {},
   "source": [
    "### Write Ensemble Results File Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904872c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Metrics ---\n",
    "def _calculate_metrics(labels_test, prediction, scores, nClasses, t_start, t_end, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of performance metrics.\n",
    "    Does NOT write any files or plot anything.\n",
    "\n",
    "    Args:\n",
    "        labels_test (np.array): True labels.\n",
    "        prediction (np.array): Predicted labels.\n",
    "        scores (np.array): Prediction scores (probabilities).\n",
    "        nClasses (int): Number of classes.\n",
    "        t_start (float): Start time for training.\n",
    "        t_end (float): End time for training.\n",
    "        model_name (str, optional): Name of the model for identification in results. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "    \"\"\"\n",
    "    metrics = {'Model Name': model_name}\n",
    "    train_time = (t_end - t_start)\n",
    "\n",
    "    if nClasses == 2:\n",
    "        positive_labels = sum(labels_test)\n",
    "        negative_labels = len(labels_test) - positive_labels\n",
    "        tn, fp, fn, tp = confusion_matrix(labels_test, prediction).ravel()\n",
    "        accuracy = accuracy_score(labels_test, prediction)\n",
    "        precision = precision_score(labels_test, prediction)\n",
    "        recall = recall_score(labels_test, prediction)\n",
    "        f1 = f1_score(labels_test, prediction)\n",
    "        roc_auc = roc_auc_score(labels_test, scores[:, 1])\n",
    "\n",
    "        metrics.update({\n",
    "            'Positive Labels': positive_labels,\n",
    "            'Negative Labels': negative_labels,\n",
    "            'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'ROC AUC': roc_auc,\n",
    "            'Training Time': train_time\n",
    "        })\n",
    "    elif nClasses > 2:\n",
    "        # For combined table, let's use overall averages\n",
    "        accuracy = accuracy_score(labels_test, prediction)\n",
    "        precision = precision_score(labels_test, prediction, average='macro', zero_division=0)\n",
    "        recall = recall_score(labels_test, prediction, average='macro', zero_division=0)\n",
    "        f1 = f1_score(labels_test, prediction, average='macro', zero_division=0)\n",
    "\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(labels_test, scores, multi_class='ovr', average='macro')\n",
    "        except ValueError:\n",
    "            roc_auc = np.nan # Or some other indicator\n",
    "\n",
    "        metrics.update({\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision (Macro)': precision,\n",
    "            'Recall (Macro)': recall,\n",
    "            'F1-score (Macro)': f1,\n",
    "            'ROC AUC (Macro OVR)': roc_auc,\n",
    "            'Training Time': train_time\n",
    "        })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# --- individual model performance: only saves ROC plots ---\n",
    "def _save_individual_model_roc_plot(labels_test, scores, nClasses, writeDir, filename_identifier=\"\", target_labels=None):\n",
    "    \"\"\"\n",
    "    Saves individual model ROC plots to the specified directory.\n",
    "    Does NOT save any CSV files.\n",
    "\n",
    "    Args:\n",
    "        labels_test (np.array): True labels.\n",
    "        scores (np.array): Prediction scores (probabilities).\n",
    "        nClasses (int): Number of classes.\n",
    "        writeDir (str): The base directory where files will be saved.\n",
    "        filename_identifier (str, optional): An identifier to prepend to filenames.\n",
    "                                            Defaults to \"\".\n",
    "        target_labels (list, optional): List of class names for multiclass plotting.\n",
    "    \"\"\"\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    \n",
    "    # Ensure the base directory exists before writing any files\n",
    "    os.makedirs(writeDir, exist_ok=True)\n",
    "\n",
    "    # Binary Classifier\n",
    "    if nClasses == 2:\n",
    "        fpr, tpr, _ = roc_curve(labels_test, scores[:, 1])\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "        roc_display.figure_.savefig(os.path.join(writeDir, f\"{filename_identifier}ROC.png\"))\n",
    "        plt.close(roc_display.figure_) # Close the figure to free memory\n",
    "\n",
    "    # Multiclass\n",
    "    elif nClasses > 2:\n",
    "        # Ensure target_labels are provided for multiclass ROC plotting\n",
    "        if target_labels is None:\n",
    "            print(\"Warning: target_labels not provided for multiclass ROC plotting. Using default names.\")\n",
    "            target_labels = [f\"Class_{i}\" for i in range(nClasses)]\n",
    "\n",
    "        for labelID in range(nClasses):\n",
    "            name = target_labels[labelID]\n",
    "            # roc_curve needs binary_truths and scores for that class\n",
    "            fpr, tpr, _ = roc_curve(labels_test[:,labelID], scores[:,labelID])\n",
    "            roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "            roc_display.figure_.savefig(os.path.join(writeDir, f\"{filename_identifier}ROC_{name}.png\"))\n",
    "            plt.close(roc_display.figure_) # Close the figure to free memory\n",
    "\n",
    "    print(f\"ROC plots saved with identifier '{filename_identifier}' in '{writeDir}'\")\n",
    "\n",
    "# --- Write Combined Ensemble Results to a Single CSV ---\n",
    "def write_combined_ensemble_results_csv(all_metrics_data, writeDir, nClasses):\n",
    "    \"\"\"\n",
    "    Writes combined performance metrics from multiple models (including ensemble)\n",
    "    into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        all_metrics_data (list): A list of dictionaries, where each dictionary\n",
    "                                 contains metrics for a model/ensemble.\n",
    "        writeDir (str): The directory where the combined CSV will be saved.\n",
    "        nClasses (int): Number of classes (to determine header).\n",
    "    \"\"\"\n",
    "    os.makedirs(writeDir, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "    combined_csv_path = os.path.join(writeDir, \"Combined_Ensemble_Results.csv\")\n",
    "\n",
    "    # Define the header based on nClasses\n",
    "    if nClasses == 2:\n",
    "        header = ['Model Name', 'Positive Labels', 'Negative Labels', 'TP', 'FP', 'TN', 'FN',\n",
    "                  'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Training Time']\n",
    "    elif nClasses > 2:\n",
    "        # The metrics dictionary from _calculate_metrics defines these for multiclass\n",
    "        header = ['Model Name', 'Accuracy', 'Precision (Macro)', 'Recall (Macro)',\n",
    "                  'F1-score (Macro)', 'ROC AUC (Macro OVR)', 'Training Time']\n",
    "    else:\n",
    "        print(f\"Warning: nClasses={nClasses} is not supported for combined CSV header.\")\n",
    "        return\n",
    "\n",
    "    with open(combined_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for metrics_dict in all_metrics_data:\n",
    "            writer.writerow(metrics_dict)\n",
    "\n",
    "    \n",
    "    print(f\"Combined ensemble results saved to: {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(net_dir, exist_ok=True)\n",
    "print(f\"Ensured base directory exists: {net_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ed961",
   "metadata": {},
   "source": [
    "### Generate validation results - Sixty/forty Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e7a10",
   "metadata": {},
   "source": [
    "#### ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f510c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(models_to_evaluate, val_ds, split_layers, nClasses, net_dir, target_labels=None):\n",
    "    print(\"--- Starting Ensemble Evaluation ---\")\n",
    "    all_ensemble_metrics = []\n",
    "\n",
    "    # 1. Prepare true labels\n",
    "    true_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "    binary_true_labels = np.argmax(true_labels, axis=1)\n",
    "\n",
    "    # 2. Gather predictions from all models (single, unified loop)\n",
    "    print(\"Gathering predictions for all models...\")\n",
    "    all_model_scores_by_batch = [[] for _ in models_to_evaluate]\n",
    "    for batch_data, _ in val_ds:\n",
    "        for idx, model in enumerate(models_to_evaluate):\n",
    "            batch_score = model(batch_data, training=False)\n",
    "            all_model_scores_by_batch[idx].append(np.array(batch_score))\n",
    "    \n",
    "    final_model_scores = [np.concatenate(scores) for scores in all_model_scores_by_batch]\n",
    "    print(\"All predictions gathered.\")\n",
    "\n",
    "    # 3. Evaluate each individual model\n",
    "    print(\"\\n--- Evaluating Individual Models ---\")\n",
    "    for idx, model_score in enumerate(final_model_scores):\n",
    "        model_name = f\"Ensemble Model {split_layers[idx]}\"\n",
    "        \n",
    "        t_start_inference = time.time()\n",
    "        model_prediction = np.argmax(model_score, axis=1)\n",
    "        t_end_inference = time.time()\n",
    "\n",
    "        _save_individual_model_roc_plot(binary_true_labels, model_score, nClasses, net_dir, f\"Model_{split_layers[idx]}_\", target_labels)\n",
    "        print(f\"'{model_name}' ROC plot saved.\")\n",
    "\n",
    "        model_metrics = _calculate_metrics(binary_true_labels, model_prediction, model_score, nClasses, t_start_inference, t_end_inference, model_name)\n",
    "        all_ensemble_metrics.append(model_metrics)\n",
    "\n",
    "    # 4. Evaluate the combined ensemble average\n",
    "    print(\"\\n--- Evaluating Ensemble Average ---\")\n",
    "    t_start_ensemble = time.time()\n",
    "    average_ensemble_score = np.mean(np.array(final_model_scores), axis=0)\n",
    "    ensemble_prediction = np.argmax(average_ensemble_score, axis=1)\n",
    "    t_end_ensemble = time.time()\n",
    "\n",
    "    _save_individual_model_roc_plot(binary_true_labels, average_ensemble_score, nClasses, net_dir, \"Ensemble_Average_\", target_labels)\n",
    "    print(\"'Ensemble Average' ROC plot saved.\")\n",
    "    \n",
    "    ensemble_metrics = _calculate_metrics(binary_true_labels, ensemble_prediction, average_ensemble_score, nClasses, t_start_ensemble, t_end_ensemble, \"Ensemble Average\")\n",
    "    all_ensemble_metrics.append(ensemble_metrics)\n",
    "\n",
    "    # 5. Write all metrics to a single CSV file\n",
    "    write_combined_ensemble_results_csv(all_ensemble_metrics, net_dir, nClasses)\n",
    "    print(f\"\\nCombined ensemble results saved successfully to '{net_dir}'.\")\n",
    "\n",
    "    return average_ensemble_score, binary_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d92e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_layers = [125, 198, 272, 385] \n",
    "trained_models = []\n",
    "\n",
    "print(\"--- Loading all trained ensemble models from disk ---\")\n",
    "\n",
    "# Loop through the split_layers to load each corresponding model\n",
    "for layer in split_layers:\n",
    "    # Re-create the exact filepath used during training\n",
    "    filepath = os.path.join(net_dir, f\"{training_style}_{layer}\")\n",
    "    modelpath = f\"{filepath}_model.keras\"\n",
    "    \n",
    "    print(f\"Loading model: {modelpath}\")\n",
    "    \n",
    "    # Load the model and add it to our list\n",
    "    model = tf.keras.models.load_model(modelpath)\n",
    "    trained_models.append(model)\n",
    "\n",
    "print(\"--- All models loaded successfully. ---\")\n",
    "\n",
    "# Call the evaluation function with the list of loaded models\n",
    "average_ensemble_score, binary_true_labels = evaluate_ensemble(\n",
    "    models_to_evaluate=trained_models,\n",
    "    val_ds=val_ds,\n",
    "    split_layers=split_layers,\n",
    "    nClasses=nClasses,\n",
    "    net_dir=net_dir,\n",
    "    target_labels=target_labels if 'target_labels' in locals() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c7e14",
   "metadata": {},
   "source": [
    "#### Probability Score Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SINGLE figure and axis for the combined plot\n",
    "fig, axs = plt.subplots(figsize=(10, 6), tight_layout=True)\n",
    "plt.rcParams.update({'font.family':'sans-serif', 'font.sans-serif':['Arial']})\n",
    "# --- using boolean masking ---\n",
    "# 1. Get scores for the NEGATIVE class where the true label is actually negative (0)\n",
    "neg_mask = (binary_true_labels == 0)\n",
    "negative_scores = average_ensemble_score[neg_mask, 1]\n",
    "\n",
    "# 2. Get scores for the POSITIVE class where the true label is actually positive (1)\n",
    "pos_mask = (binary_true_labels == 1)\n",
    "positive_scores = average_ensemble_score[pos_mask, 1]\n",
    "\n",
    "# --- Plot both histograms on the same axes ---\n",
    "# Plot the histogram for the negative class\n",
    "axs.hist(negative_scores, bins=20, color='skyblue', range=(0, 1), edgecolor='black', alpha=0.7, label='Negative Labels')\n",
    "# Plot the histogram for the positive class\n",
    "axs.hist(positive_scores, bins=20, color='salmon', range=(0, 1), edgecolor='black', alpha=0.7, label='Positive Labels')\n",
    "\n",
    "# --- Formatting ---\n",
    "# # Remove axes splines for a cleaner look\n",
    "# for s in ['top', 'right']:\n",
    "#     axs.spines[s].set_visible(False)\n",
    "\n",
    "# Add gridlines\n",
    "# Set the tick locations for the grid\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "# Dynamically set y-ticks based on histogram height\n",
    "_, y_max = plt.ylim()\n",
    "plt.yticks(np.arange(0, y_max + 2, 2))\n",
    "# axs.grid(axis='y', color='grey', linestyle='-.', linewidth=0.5, alpha=0.6)\n",
    "# _, y_max = plt.ylim()\n",
    "# plt.yticks(np.arange(0, y_max + 1, 2))\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add labels, title, and a legend\n",
    "axs.axvline(0.5, color='red', linestyle='--', linewidth=2, label='0.5 Threshold')\n",
    "plt.xlabel('Predicted Probability Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Model Confidence')\n",
    "plt.legend()\n",
    "save_path_pos = os.path.join(net_dir, 'histogram_all_labels_combined.png')\n",
    "plt.savefig(save_path_pos, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Filter the data based on the TRUE labels ---\n",
    "# Filter for all truly NEGATIVE samples (includes True Negatives and False Positives)\n",
    "neg_true_mask = (binary_true_labels == 0)\n",
    "# Get the model's predicted probability of being POSITIVE for these samples\n",
    "scores_for_neg_labels = average_ensemble_score[neg_true_mask, 1]\n",
    "\n",
    "# Filter for all truly POSITIVE samples (includes True Positives and False Negatives)\n",
    "pos_true_mask = (binary_true_labels == 1)\n",
    "# Get the model's predicted probability of being POSITIVE for these samples\n",
    "scores_for_pos_labels = average_ensemble_score[pos_true_mask, 1]\n",
    "\n",
    "# --- 2. Create the plots separately ---\n",
    "# == Plot 1: For Negative Labels (TN + FP) ==\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores_for_neg_labels, bins=20, range=(0, 1), color='skyblue', edgecolor='black')\n",
    "plt.axvline(0.5, color='red', linestyle='--', linewidth=2, label='0.5 Threshold')\n",
    "\n",
    "# Set the tick locations for the grid\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "# Dynamically set y-ticks based on histogram height\n",
    "_, y_max = plt.ylim()\n",
    "plt.yticks(np.arange(0, y_max + 2, 2))\n",
    "\n",
    "# Apply the styled grid to both axes\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.title('Predictions for Negative Labels (TN + FP)', fontweight='bold')\n",
    "plt.xlabel('Predicted Probability of Being Positive')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "save_path_neg = os.path.join(net_dir, 'histogram_negative_labels.png')\n",
    "plt.savefig(save_path_neg, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# == Plot 2: For Positive Labels (TP + FN) ==\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores_for_pos_labels, bins=20, range=(0, 1), color='salmon', edgecolor='black')\n",
    "plt.axvline(0.5, color='red', linestyle='--', linewidth=2, label='0.5 Threshold')\n",
    "\n",
    "# Set the tick locations for the grid\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "# Dynamically set y-ticks based on histogram height\n",
    "_, y_max = plt.ylim()\n",
    "plt.yticks(np.arange(0, y_max + 2, 2))\n",
    "\n",
    "# Apply the styled grid to both axes\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.title('Predictions for Positive Labels (TP + FN)', fontweight='bold')\n",
    "plt.xlabel('Predicted Probability of Being Positive')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Save and show the plot\n",
    "save_path_pos = os.path.join(net_dir, 'histogram_positive_labels.png')\n",
    "plt.savefig(save_path_pos, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gridded plots saved successfully to '{net_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd135e94",
   "metadata": {},
   "source": [
    "#### GET POSITIVE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_val_dir = val_dir + \"//2R\"\n",
    "# neg_val_dir = val_dir + \"//0-1R\"\n",
    "\n",
    "# neg_filepath = os.walk(neg_val_dir)\n",
    "# pos_filepath = os.walk(pos_val_dir)\n",
    "\n",
    "# pos_labels = []\n",
    "# neg_labels = []\n",
    "\n",
    "# for path, _, file_names in os.walk(neg_val_dir): \n",
    "#     neg_file_names = file_names\n",
    "#     for file in file_names:\n",
    "#         ecg = Image.open(path+'//'+file)\n",
    "#         ecg = np.expand_dims(ecg,axis=0)\n",
    "#         all_predictions = []\n",
    "#         for model in trained_models:\n",
    "#             print(f\"  - Predicting with model...\")\n",
    "#             preds = model.predict(ecg)\n",
    "#             all_predictions.append(preds)\n",
    "#         neg_labels.append(all_predictions)\n",
    "# neg_labels = np.array(neg_labels)\n",
    "\n",
    "# for file in pos_filepath: \n",
    "#     ecg = Image.open(file)\n",
    "#     ecg = np.expand_dims(ecg,axis=0)\n",
    "#     all_predictions = []\n",
    "#     for model in trained_models:\n",
    "#         print(f\"  - Predicting with model...\")\n",
    "#         preds = model.predict(ecg)\n",
    "#         all_predictions.append(preds)\n",
    "#     pos_labels.append(all_predictions)\n",
    "# pos_labels = np.array(pos_labels)\n",
    "\n",
    "\n",
    "# # == TO FIND TRUE POSITIVES ==== \n",
    "# # Extract the true labels from the dataset so we can calculate true positives\n",
    "# print(\" - Extracting true labels from the dataset...\")\n",
    "# true_labels = []\n",
    "# file_paths = []\n",
    "# for batches, paths in unshuffled_val_ds:\n",
    "#         true_labels.append(batches[1])\n",
    "#         file_paths.append(paths)\n",
    "\n",
    "# true_labels_categorical = np.concatenate([y for x, y in unshuffled_val_ds], axis=0) # This is in a shape of (batch_size, classes) \n",
    "# true_label_indices = np.argmax(true_labels_categorical, axis=1) # The same as ensemble indices \n",
    "\n",
    "# all_predictions = []\n",
    "# print(\"\\n--- Getting predictions from each loaded model ---\")\n",
    "\n",
    "# # 1. Loop through loaded models to get their individual predictions\n",
    "# for model in trained_models:\n",
    "#     print(f\"  - Predicting with model...\")\n",
    "#     preds = model.predict(unshuffled_val_ds)\n",
    "#     all_predictions.append(preds)\n",
    "\n",
    "# print(\"\\n--- Averaging predictions for the final ensemble result ---\")\n",
    "# # 2. Average the predictions to get the final ensemble decision\n",
    "# ensemble_predictions = np.mean(np.array(all_predictions), axis=0) # The prediction has a shape of (models, images, classes), axis=0 is the models \n",
    "# ensemble_predicted_indices = np.argmax(ensemble_predictions, axis=1) # The input is a 2D array with a shape of (images, class_probabilities)\n",
    "\n",
    "# # 4. Define the positive class index \n",
    "# print(\"\\n--- Identifying positive class ---\")\n",
    "\n",
    "# positive_class_name = None\n",
    "# if Dataset == \"Binary-1\":\n",
    "#     positive_class_name = '2R'\n",
    "# elif Dataset == \"Binary-2\":\n",
    "#     positive_class_name = '1-2R'\n",
    "# else: # This handles the case where the Dataset variable is something unexpected\n",
    "#     print(f\"FATAL: Unrecognised dataset name '{Dataset}'. Please check the variable. Exiting.\")\n",
    "#     exit()\n",
    "# try: # This safely finds the index of the class name\n",
    "#     positive_class_index = class_names.index(positive_class_name)\n",
    "#     print(f\"Successfully identified positive class '{positive_class_name}' at index {positive_class_index}.\")\n",
    "# except ValueError:\n",
    "#     print(f\"FATAL: The positive class '{positive_class_name}' was not found in the dataset's classes: {class_names}. Exiting.\")\n",
    "#     exit()\n",
    "\n",
    "# # ======================== DEBUGGING SANITY CHECK ========================\n",
    "# print(\"\\n--- Sanity Check: Comparing first 15 predictions to true labels ---\")\n",
    "# for i in range(110):\n",
    "#     # Get the data for the i-th image\n",
    "#     filename = os.path.basename(image_filenames[i])\n",
    "#     predicted_class_index = ensemble_predicted_indices[i]\n",
    "#     true_class_index = true_label_indices[i]\n",
    "    \n",
    "#     # Convert indices to readable class names\n",
    "#     predicted_class_name = class_names[predicted_class_index]\n",
    "#     true_class_name = class_names[true_class_index]\n",
    "    \n",
    "#     # Check if this specific instance is a True Positive\n",
    "#     is_tp_text = \"\"\n",
    "#     if predicted_class_index == positive_class_index and true_class_index == positive_class_index:\n",
    "#         is_tp_text = \"TRUE POSITIVE\"\n",
    "\n",
    "#     print(f\"File: {filename:<20} | Predicted: '{predicted_class_name}' | Actual: '{true_class_name}' {is_tp_text}\")\n",
    "# print(\"--- End of Sanity Check ---\\n\")\n",
    "# # ========================================================================\n",
    "\n",
    "# # 5. Identify which predictions were Positive and which were True Positive\n",
    "# positive_images = []\n",
    "# true_positive_images = []\n",
    "\n",
    "# for i, filename in enumerate(image_filenames):\n",
    "#     predicted_index = ensemble_predicted_indices[i]\n",
    "#     true_index = true_label_indices[i]\n",
    "#     # Check for any image the model PREDICTED as positive\n",
    "#     if predicted_index == positive_class_index:\n",
    "#         positive_images.append(os.path.basename(filename))\n",
    "#     # Check for images that were PREDICTED positive AND ARE ACTUALLY positive\n",
    "#     if predicted_index == positive_class_index and true_index == positive_class_index:\n",
    "#         true_positive_images.append(os.path.basename(filename))\n",
    "\n",
    "# # 6. Report final results and save the True Positive list to a file\n",
    "# print(\"\\n--- Final Results ---\")\n",
    "\n",
    "# # Get the final counts\n",
    "# num_predicted_positive = len(positive_images)\n",
    "# num_true_positive = len(true_positive_images)\n",
    "\n",
    "# print(f\"Total Subjects Classified as Positive: {num_predicted_positive}\")\n",
    "# print(f\"Total Subjects that are True Positives: {num_true_positive}\")\n",
    "\n",
    "# # Check if any true positives were found and list them\n",
    "# if true_positive_images:\n",
    "#     print(\"\\nThe following subjects were correctly identified as 'positive' (True Positives):\")\n",
    "#     # Sort the list for clean, repeatable output\n",
    "#     for fname in sorted(true_positive_images):\n",
    "#         print(f\"- {fname}\")\n",
    "#     # Save the TRUE POSITIVES list to an Excel file\n",
    "#     try:\n",
    "#         df = pd.DataFrame(sorted(true_positive_images), columns=['True_Positive_Filename'])\n",
    "#         output_excel_path = os.path.join(net_dir, 'TRUE_POSITIVES_results.xlsx')\n",
    "#         df.to_excel(output_excel_path, index=False)\n",
    "#         print(f\"\\nTrue Positive list successfully saved to: {output_excel_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nCould not save Excel file. Error: {e}\")\n",
    "# else:\n",
    "#     print(\"\\n No subjects were correctly identified as 'positive' (No True Positives).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af6560",
   "metadata": {},
   "source": [
    "#### Original ensemble results cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ORIGINAL SCRIPT:\n",
    "# # ==== Ensemble ImageNet Pretrained & AFIB/MI Pretrained =====\n",
    "# if Pretrained_model == \"ENB3\": \n",
    "#     split_layers = [125, 198, 272, 385]\n",
    "#     results = []\n",
    "    \n",
    "#     for split_layer in split_layers:\n",
    "#         cutModel = tf.keras.Model(ENB3_model.input, ENB3_model.layers[split_layer-1].output)\n",
    "#         EnsembleENB3_model = add_classification_layers(cutModel)\n",
    "\n",
    "#         filepath = os.path.join(net_dir, f\"{training_style}_{split_layer}\")\n",
    "#         modelpath = filepath + \"_\" + \"model.keras\"\n",
    "        \n",
    "#         print(f\"--- Training model truncated at layer {split_layer} --- \")\n",
    "#         t_start_model = time.time() \n",
    "#         results.append(training_function(EnsembleENB3_model, modelpath, train_ds, val_ds)) #softmax \n",
    "#         t_end_model = time.time() \n",
    "#         # results = training_function(EnsembleENB3_model, modelpath, train_ds, val_ds)\n",
    "    \n",
    "#     print(\"--- All training complete. Generating combined graph. ---\")\n",
    "#     combined_filepath = os.path.join(net_dir, training_style)\n",
    "#     # graph_all_losses(results, split_layers, combined_filepath)\n",
    "#     # graph_loss_function(results, filepath)\n",
    "\n",
    "# else:\n",
    "#     split_layers = [125, 198, 272, 385]\n",
    "#     results = [] # This will store all the history objects\n",
    "#     graph_labels = [] # This will store labels for each plot line\n",
    "\n",
    "#     print(\"--- Starting Ensemble Training ---\")\n",
    "    \n",
    "#     for idx, split_layer in enumerate(split_layers): \n",
    "#         loaded_model = pretrained_ensemble_models[idx]\n",
    "#         cutModel = tf.keras.Model(loaded_model.input, loaded_model.layers[split_layer-1].output)\n",
    "#         full_model = add_classification_layers(cutModel)\n",
    "#         pretrained_ensemble_models[idx] = full_model\n",
    "        \n",
    "#         filepath = os.path.join(net_dir, f\"{training_style}_{split_layer}\") # filepath = net_dir + \"//\" + training_style + \"_\" + str(split_layer)\n",
    "#         modelpath = filepath + \"_\" + \"model.keras\"\n",
    "        \n",
    "#         # Run training and append the history object to the results list\n",
    "#         print(f\"--- Training model truncated at layer {split_layer} --- \")\n",
    "#         t_start_model = time.time() # Start time for training\n",
    "#         results.append(training_function(pretrained_ensemble_models[idx], modelpath, train_ds, val_ds)) #softmax \n",
    "#         t_end_model = time.time() # End time for training\n",
    "    \n",
    "#     print(\"--- All training complete. Generating combined graph. ---\")\n",
    "    \n",
    "#     # Call the graphing function, passing the list of split_layers for color mapping\n",
    "#     combined_filepath = net_dir + \"//\" + training_style\n",
    "#     # graph_all_losses(results, split_layers, combined_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ffb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USE THIS SCRIPT FOR ALL ENSEMBLE MODEL RESTULTS #\n",
    "# print(\"--- Starting Ensemble Evaluation ---\")\n",
    "\n",
    "# all_ensemble_metrics = [] # List to store metrics dictionary for each model and the ensemble\n",
    "\n",
    "# # Get all true labels from the validation set \n",
    "# true_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "# binary_true_labels = np.argmax(true_labels, axis=1)\n",
    "\n",
    "# # Get predictions for all models in a single pass over the data.\n",
    "# print(\"Gathering predictions for all models...\")\n",
    "# if Pretrained_model == \"ENB3\": \n",
    "#     all_model_scores_by_batch = [[] for _ in EnsembleENB3_model]\n",
    "#     for batch_data, _ in val_ds:\n",
    "#         for idx, model in enumerate(EnsembleENB3_model):\n",
    "#             batch_score = model(batch_data, training=False) \n",
    "#             all_model_scores_by_batch[idx].append(np.array(batch_score))\n",
    "# else: \n",
    "#     all_model_scores_by_batch = [[] for _ in pretrained_ensemble_models]\n",
    "#     for batch_data, _ in val_ds:\n",
    "#         for idx, model in enumerate(pretrained_ensemble_models):\n",
    "#             batch_score = model(batch_data, training=False) \n",
    "#             all_model_scores_by_batch[idx].append(np.array(batch_score))\n",
    "\n",
    "# # Concatenate the batch scores for each model into a single array\n",
    "# final_model_scores = [np.concatenate(scores) for scores in all_model_scores_by_batch]\n",
    "# print(\"All predictions gathered.\")\n",
    "\n",
    "# # --- 1. Evaluate each individual model ---\n",
    "# print(\"\\n--- Evaluating Individual Models ---\")\n",
    "# for idx, model_score in enumerate(final_model_scores):\n",
    "#     split_layer = split_layers[idx]\n",
    "#     model_name = f\"Ensemble Model {split_layer}\"\n",
    "\n",
    "#     # Time the inference step (argmax) for this model's predictions\n",
    "#     t_start_inference = time.time()\n",
    "#     model_prediction = np.argmax(model_score, axis=1)\n",
    "#     t_end_inference = time.time()\n",
    "\n",
    "#     # Save individual ROC plot for this model\n",
    "#     _save_individual_model_roc_plot(\n",
    "#         binary_true_labels, model_score, nClasses,\n",
    "#         writeDir=net_dir, filename_identifier=f\"Model_{split_layer}_\",\n",
    "#         target_labels=target_labels if 'target_labels' in locals() else None\n",
    "#     )\n",
    "#     print(f\"'{model_name}' ROC plot saved.\")\n",
    "\n",
    "#     # Calculate metrics for this individual model\n",
    "#     model_metrics = _calculate_metrics(\n",
    "#         binary_true_labels, model_prediction, model_score, nClasses,\n",
    "#         t_start=t_start_inference,\n",
    "#         t_end=t_end_inference,\n",
    "#         model_name=model_name\n",
    "#     )\n",
    "#     all_ensemble_metrics.append(model_metrics)\n",
    "\n",
    "# # --- 2. Evaluate the combined ensemble average ---\n",
    "# print(\"\\n--- Evaluating Ensemble Average ---\")\n",
    "\n",
    "# # Time the ensemble averaging and prediction step\n",
    "# t_start_ensemble = time.time()\n",
    "# average_ensemble_score = np.mean(np.array(final_model_scores), axis=0)\n",
    "# ensemble_prediction = np.argmax(average_ensemble_score, axis=1)\n",
    "# t_end_ensemble = time.time()\n",
    "\n",
    "# # Save ROC plot for the ensemble average\n",
    "# _save_individual_model_roc_plot(\n",
    "#     binary_true_labels, average_ensemble_score, nClasses,\n",
    "#     writeDir=net_dir, filename_identifier=\"Ensemble_Average_\",\n",
    "#     target_labels=target_labels if 'target_labels' in locals() else None\n",
    "# )\n",
    "# print(\"'Ensemble Average' ROC plot saved.\")\n",
    "\n",
    "# # Calculate metrics for the ensemble average\n",
    "# ensemble_metrics = _calculate_metrics(\n",
    "#     binary_true_labels, ensemble_prediction, average_ensemble_score, nClasses,\n",
    "#     t_start=t_start_ensemble,\n",
    "#     t_end=t_end_ensemble,\n",
    "#     model_name=\"Ensemble Average\"\n",
    "# )\n",
    "# all_ensemble_metrics.append(ensemble_metrics)\n",
    "\n",
    "# # --- 3. Write all collected metrics to a single CSV file ---\n",
    "# write_combined_ensemble_results_csv(all_ensemble_metrics, net_dir, nClasses)\n",
    "# print(f\"\\n Combined ensemble results saved successfully to '{net_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1216334",
   "metadata": {},
   "source": [
    "#### SWOIP OR ENSEMBLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original script: \n",
    "# if training_style == \"SWOIP\": \n",
    "#     temp_score = []\n",
    "#     temp_labels = []\n",
    "#     for batch, label in val_ds:\n",
    "#         batch_score = SWOIP_model(batch)\n",
    "#         temp_score.append (np.array(batch_score))\n",
    "#         temp_labels.append (np.array (label))\n",
    "#     model_score = np.concatenate(temp_score)\n",
    "#     true_labels = np.concatenate(temp_labels)\n",
    "#     model_prediction = np.argmax(model_score, axis=1)\n",
    "#     binary_true_labels = np.argmax (true_labels, axis=1)\n",
    "    \n",
    "#     # Save individual results for SWOIP\n",
    "#     writeResultsFile(\n",
    "#         binary_true_labels, model_prediction, model_score, nClasses, \n",
    "#         t_start = 0 , t_end = 0, writeDir=net_dir,\n",
    "#     )\n",
    "#     print(\"SWOIP individual results saved.\")\n",
    "\n",
    "# elif training_style == \"Ensemble\":\n",
    "#     all_ensemble_metrics = [] # List to store metrics for all models and ensemble\n",
    "#     ensemble_scores_list = [] # To collect scores for ensemble averaging\n",
    "\n",
    "#     # Assume true_labels and binary_true_labels are consistent across all batches for val_ds. Initialize outside the loop if val_ds is consistently processed\n",
    "#     first_batch, first_label = next(iter(val_ds)) # Get first batch to determine true_labels structure\n",
    "#     true_labels_template = np.concatenate([np.array(l) for _, l in val_ds]) # Get all true labels\n",
    "#     binary_true_labels_template = np.argmax(true_labels_template, axis=1)\n",
    "\n",
    "#     for idx, split_layer in enumerate(split_layers):\n",
    "#         temp_score = []\n",
    "#         for batch, label in val_ds: # Re-iterate val_ds to get scores for current model\n",
    "#             batch_score = pretrained_ensemble_models[idx](batch)\n",
    "#             temp_score.append (np.array(batch_score))\n",
    "#         model_score = np.concatenate(temp_score)\n",
    "\n",
    "#         # model_prediction is derived from model_score for this specific model\n",
    "#         model_prediction = np.argmax(model_score, axis=1)\n",
    "\n",
    "#         # Save individual ROC plot for this model\n",
    "#         _save_individual_model_roc_plot(\n",
    "#             binary_true_labels_template, model_score, nClasses,\n",
    "#             writeDir= net_dir, filename_identifier=f\"Model_{split_layer}_\",\n",
    "#             target_labels=target_labels if 'target_labels' in globals() else None\n",
    "#         )\n",
    "#         print(f\"Ensemble Model {split_layer} ROC plot saved.\")\n",
    "\n",
    "#         # Calculate metrics for this individual model and add to list\n",
    "#         model_metrics = _calculate_metrics(\n",
    "#             binary_true_labels_template, model_prediction, model_score, nClasses,\n",
    "#             t_start = t_start_model,    # Pass the actual start time\n",
    "#             t_end = t_end_model,         # Pass the actual end time\n",
    "#             model_name=f\"Ensemble Model {split_layer}\"\n",
    "#         )\n",
    "#         all_ensemble_metrics.append(model_metrics)\n",
    "#         ensemble_scores_list.append(model_score)\n",
    "\n",
    "#     # Calculate ensemble average performance\n",
    "#     ensemble_scores_array = np.array(ensemble_scores_list)\n",
    "#     average_ensemble_score = np.mean(ensemble_scores_array, axis=0) # Corrected axis\n",
    "#     ensemble_prediction = np.argmax(average_ensemble_score, axis=1)\n",
    "\n",
    "#     # Save individual ROC plot for the ensemble average\n",
    "#     _save_individual_model_roc_plot(\n",
    "#         binary_true_labels_template, average_ensemble_score, nClasses,\n",
    "#         writeDir= net_dir, filename_identifier=\"Ensemble_Average_\",\n",
    "#         target_labels=target_labels if 'target_labels' in globals() else None\n",
    "#     )\n",
    "#     print(\"Ensemble Average ROC plot saved.\")\n",
    "\n",
    "#     # Calculate metrics for the ensemble average and add to list\n",
    "#     ensemble_metrics = _calculate_metrics(\n",
    "#         binary_true_labels_template, ensemble_prediction, average_ensemble_score, nClasses,\n",
    "#         t_start=t_start_model, \n",
    "#         t_end=t_end_model,\n",
    "#         model_name=\"Ensemble Average\"\n",
    "#     )\n",
    "#     all_ensemble_metrics.append(ensemble_metrics)\n",
    "\n",
    "#     # Finally, write all collected metrics to a single combined CSV\n",
    "#     write_combined_ensemble_results_csv(all_ensemble_metrics, net_dir, nClasses)\n",
    "#     print(\"Combined ensemble results CSV saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60285f6d",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # results = fullmodel.evaluate (test_ds)\n",
    "# # print (results)\n",
    "\n",
    "# temp_score = []\n",
    "# temp_labels = []\n",
    "\n",
    "# for batch, label in test_ds:\n",
    "#     batch_score = SWOIP_model(batch)\n",
    "#     temp_score.append (np.array(batch_score))\n",
    "#     temp_labels.append (np.array (label))\n",
    "\n",
    "# model_score = np.concatenate(temp_score)\n",
    "# true_labels = np.concatenate(temp_labels)\n",
    "\n",
    "# # print (model_score)\n",
    "# # print (true_labels)\n",
    "# # print (model_score.shape) \n",
    "\n",
    "# model_prediction = np.argmax(model_score, axis=1)\n",
    "# binary_true_labels = np.argmax (true_labels, axis=1)\n",
    "\n",
    "# writeResultsFile(binary_true_labels, model_prediction, model_score, nClasses, t_start = 0 , t_end = 0, writeDir=net_dir)\n",
    "\n",
    "# # print(model_prediction)\n",
    "# # print(len(model_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94edc492",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
