{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports \n",
    "2. Parameters and variables \n",
    "3. Data imports \n",
    "4. Load Pre-trained model \n",
    "5. Model building functions \n",
    "6. Cross validation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay, roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import csv\n",
    "import time \n",
    "from pathlib import Path\n",
    "\n",
    "# print (tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING PARAMETERS & VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Parameters ###\n",
    "seed = 2806\n",
    "batch_size = 8 #16 \n",
    "# learning_rate = 0.0001\n",
    "max_epochs = 100\n",
    "# patience = 15\n",
    "image_size = 300\n",
    "img_width, img_height = image_size, image_size\n",
    "\n",
    "### Label Information ### \n",
    "# target_labels = ['mild rejection','moderate-to-severe rejection','no rejection']\n",
    "# classifier = 'binary'\n",
    "training_style = \"Ensemble\"\n",
    "Dataset = \"Binary-1\"\n",
    "Pretrained_model = \"ENB3\"\n",
    "\n",
    "# ==== Create target directory for saving/loading models === \n",
    "project_dir = 'D://Rui//'\n",
    "\n",
    "# === Create net directory for saving/loading models === \n",
    "date = datetime.now()\n",
    "net_dir = project_dir + 'Transplant_prediction_models//' + training_style + Pretrained_model + '_' + Dataset + '_' + \"5_folds\" + '_' + str(date.day).rjust(2, '0')+str(date.month).rjust(2, '0')+str(date.year)+'_'+ str(date.hour)+str(date.minute)\n",
    "os.mkdir(net_dir)\n",
    "\n",
    "if Dataset == \"Binary-1\":\n",
    "    target_labels = ['0-1R', \"2R\"]\n",
    "    nClasses = 2\n",
    "elif Dataset == \"Binary-2\":\n",
    "    target_labels = ['0', \"1-2R\"]\n",
    "    nClasses = 2\n",
    "\n",
    "# === Pre-trained Net Location ===\n",
    "if Pretrained_model == \"AF\":\n",
    "    pretrained_net_dir = project_dir + 'AFIBvsNOTNet(080120254x3cols)//'\n",
    "elif Pretrained_model == \"MI\":\n",
    "    pretrained_net_dir = project_dir + 'MulticlassMIPTBXLTraining(4x3)//'\n",
    "\n",
    "print(\"target_labels =\", target_labels)\n",
    "print(\"nClasses = \", nClasses)\n",
    "# print(pretrained_net_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input Folders\n",
    "if Dataset == \"Binary-1\":\n",
    "    dataset_dir = \"D://Rui//NIDACT2 Transplant ECGs//Multiclass_HTx_Rejection_Dataset_1(300_300)//Binary1_0-1Rvs2R (5-Folds)\"\n",
    "elif Dataset == \"Binary-2\": \n",
    "    dataset_dir = \"D://Rui//NIDACT2 Transplant ECGs//Multiclass_HTx_Rejection_Dataset_1(300_300)//Binary2_0vs1-2R (5-Folds)\"\n",
    "\n",
    "print(\"dataset_dir =\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFDS Import & Organise Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert the string to a Path object\n",
    "dataset_path = Path(dataset_dir)\n",
    "# 2. Iterate over the contents of that path\n",
    "fold_dirs = [d for d in dataset_path.iterdir() if d.is_dir()]\n",
    "# Get the number of folds\n",
    "num_folds = len(fold_dirs)\n",
    "print(f\"Found {num_folds} fold directories.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS ONE script \n",
    "def add_classification_layers(cutModel): #l1_reg, l2_reg, dropout_rate_1, dropout_rate_2, nClasses\n",
    "    classifier = cutModel.output\n",
    "    classifier = tf.keras.layers.GlobalAveragePooling2D()(classifier)\n",
    "    # --- First dense layer with customizable regularization and dropout ---\n",
    "    classifier = tf.keras.layers.Dense(128, activation=\"relu\", \n",
    "                                     kernel_regularizer=regularizers.L1L2(l1=0.0001, l2=0.0001))(classifier) #l2_reg\n",
    "    classifier = tf.keras.layers.Dropout(0.4)(classifier) #dropout_rate_1\n",
    "    # --- Second dense layer with customizable regularization and dropout ---\n",
    "    classifier = tf.keras.layers.Dense(64, activation=\"relu\",\n",
    "                                     kernel_regularizer=regularizers.L1L2(l1=0.0001, l2=0.0001))(classifier)\n",
    "    classifier = tf.keras.layers.Dropout(0.5)(classifier) #dropout_rate_2\n",
    "    classifier = tf.keras.layers.Dense(nClasses, activation=\"softmax\")(classifier)\n",
    "    full_model = tf.keras.Model(inputs=cutModel.input, outputs=classifier)\n",
    "    return full_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS ONE script \n",
    "def training_function(full_model, filepath, train_ds, val_ds): \n",
    "    # Define optimizer with an initial learning rate \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001) # This is my initial learning rate \n",
    "\n",
    "    # Define ReduceLROnPlateau callback (based on validation loss)\n",
    "    reduce_lr_patience = 10\n",
    "    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2, # Reduce LR by a factor of 5 (1/5 = 0.2)\n",
    "        patience=reduce_lr_patience,\n",
    "        min_lr=0.00001, # Don't let the LR get ridiculously small\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Early stopping \n",
    "    early_stopping_patience = 25 #50\n",
    "    early_stopper = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=early_stopping_patience,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True # Automatically restore the model from the best epoch\n",
    "    )\n",
    "\n",
    "    saving_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, \n",
    "        monitor=\"val_loss\", \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=False, \n",
    "        save_freq=\"epoch\",\n",
    "        initial_value_threshold=None\n",
    "    )\n",
    "\n",
    "    full_model.compile(optimizer=optimizer, \n",
    "                    loss = tf.keras.losses.BinaryCrossentropy(), #CategoricalFocalCrossentropy\n",
    "                    metrics=['binary_accuracy'])\n",
    "    \n",
    "    # Calculate class weights \n",
    "    labels_train = []\n",
    "    for _ , label in train_ds:\n",
    "        labels_train.append (np.array (label))\n",
    "    labels_train = np.concatenate(labels_train)\n",
    "    weight_function = lambda i: (1/sum(labels_train[:,i])*(labels_train.shape[0]/labels_train.shape[1]))\n",
    "    class_weights = {i: weight_function(i) for i in range(labels_train.shape[1])}\n",
    "    # print(class_weights)\n",
    "\n",
    "    # Training \n",
    "    results = full_model.fit(train_ds,\n",
    "                        batch_size=batch_size, \n",
    "                        validation_data=val_ds, \n",
    "                        epochs=max_epochs,\n",
    "                        callbacks=[early_stopper, lr_callback, saving_callback], # early_stopper, lr_callback, \n",
    "                        class_weight = class_weights\n",
    "                        )\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fold_results(labels_test, ensemble_prediction, ensemble_scores, nClasses, t_start, t_end, fold_number=None, output_base_dir=None):\n",
    "    \"\"\"\n",
    "    Calculates metrics for a single fold and returns them as a dictionary.\n",
    "    Optionally saves ROC curve if output_base_dir is provided.\n",
    "    \"\"\"\n",
    "    results = {} # Initialize results dictionary as empty\n",
    "    # FYI: The output_base_dir parameter already carries the correct path.\n",
    "    # Binary Classifier (nClasses == 2)\n",
    "    if nClasses == 2:\n",
    "        positive_labels = sum(labels_test)\n",
    "        negative_labels = len(labels_test) - positive_labels\n",
    "        tn, fp, fn, tp = confusion_matrix(labels_test, ensemble_prediction).ravel()\n",
    "        accuracy = accuracy_score(labels_test, ensemble_prediction)\n",
    "        precision = precision_score(labels_test, ensemble_prediction)\n",
    "        recall = recall_score(labels_test, ensemble_prediction)\n",
    "        f1 = f1_score(labels_test, ensemble_prediction)\n",
    "        train_time = (t_end - t_start)\n",
    "        fpr, tpr, _ = roc_curve(labels_test, ensemble_scores[:, 1])\n",
    "        roc_auc = roc_auc_score(labels_test, ensemble_scores[:, 1])\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        results = {\n",
    "            'Fold': fold_number if fold_number is not None else 'Overall',\n",
    "            'Positive Labels': int(positive_labels),\n",
    "            'Negative Labels': int(negative_labels),\n",
    "            'TP': int(tp),\n",
    "            'FP': int(fp),\n",
    "            'TN': int(tn),\n",
    "            'FN': int(fn),\n",
    "            'Accuracy': float(accuracy),\n",
    "            'Precision': float(precision),\n",
    "            'Recall': float(recall),\n",
    "            'F1-score': float(f1),\n",
    "            'ROC AUC': float(roc_auc),\n",
    "            'Training Time (s)': float(train_time)\n",
    "        }\n",
    "\n",
    "        if output_base_dir:\n",
    "            os.makedirs(output_base_dir, exist_ok=True)\n",
    "            roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "            if fold_number is not None:\n",
    "                roc_display.figure_.savefig(f\"{output_base_dir}/ROC_Fold_{fold_number}.png\")\n",
    "            else:\n",
    "                roc_display.figure_.savefig(f\"{output_base_dir}/ROC_Overall.png\")\n",
    "            plt.close(roc_display.figure_)\n",
    "\n",
    "    # Multi-class Classifier (nClasses > 2)\n",
    "    elif nClasses > 2:\n",
    "        accuracy = accuracy_score(labels_test, ensemble_prediction)\n",
    "        f1_macro = f1_score(labels_test, ensemble_prediction, average='macro')\n",
    "        precision_macro = precision_score(labels_test, ensemble_prediction, average='macro')\n",
    "        recall_macro = recall_score(labels_test, ensemble_prediction, average='macro')\n",
    "        train_time = (t_end - t_start)\n",
    "\n",
    "        results = {\n",
    "            'Fold': fold_number if fold_number is not None else 'Overall',\n",
    "            'Accuracy': float(accuracy),\n",
    "            'Precision (Macro)': float(precision_macro),\n",
    "            'Recall (Macro)': float(recall_macro),\n",
    "            'F1-score (Macro)': float(f1_macro),\n",
    "            'Training Time (s)': float(train_time)\n",
    "        }\n",
    "        if output_base_dir:\n",
    "            # Multi-class ROC is more complex, so this part is often left out or implemented differently\n",
    "            print(f\"Note: ROC curve generation for multi-class (nClasses={nClasses}) not fully implemented here.\")\n",
    "            # If you need multi-class ROC, you'd add specific logic here,\n",
    "            # potentially using OneVsRestClassifier or similar strategies.\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Metrics ---\n",
    "def _calculate_metrics(labels_test, prediction, scores, nClasses, t_start, t_end, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of performance metrics.\n",
    "    Does NOT write any files or plot anything.\n",
    "\n",
    "    Args:\n",
    "        labels_test (np.array): True labels.\n",
    "        prediction (np.array): Predicted labels.\n",
    "        scores (np.array): Prediction scores (probabilities).\n",
    "        nClasses (int): Number of classes.\n",
    "        t_start (float): Start time for training.\n",
    "        t_end (float): End time for training.\n",
    "        model_name (str, optional): Name of the model for identification in results. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated metrics.\n",
    "    \"\"\"\n",
    "    metrics = {'Model Name': model_name}\n",
    "    train_time = (t_end - t_start)\n",
    "\n",
    "    if nClasses == 2:\n",
    "        positive_labels = sum(labels_test)\n",
    "        negative_labels = len(labels_test) - positive_labels\n",
    "        tn, fp, fn, tp = confusion_matrix(labels_test, prediction).ravel()\n",
    "        accuracy = accuracy_score(labels_test, prediction)\n",
    "        precision = precision_score(labels_test, prediction)\n",
    "        recall = recall_score(labels_test, prediction)\n",
    "        f1 = f1_score(labels_test, prediction)\n",
    "        roc_auc = roc_auc_score(labels_test, scores[:, 1])\n",
    "\n",
    "        metrics.update({\n",
    "            'Positive Labels': positive_labels,\n",
    "            'Negative Labels': negative_labels,\n",
    "            'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'ROC AUC': roc_auc,\n",
    "            'Training Time': train_time\n",
    "        })\n",
    "    elif nClasses > 2:\n",
    "        # For combined table, let's use overall averages\n",
    "        accuracy = accuracy_score(labels_test, prediction)\n",
    "        precision = precision_score(labels_test, prediction, average='macro', zero_division=0)\n",
    "        recall = recall_score(labels_test, prediction, average='macro', zero_division=0)\n",
    "        f1 = f1_score(labels_test, prediction, average='macro', zero_division=0)\n",
    "\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(labels_test, scores, multi_class='ovr', average='macro')\n",
    "        except ValueError:\n",
    "            roc_auc = np.nan # Or some other indicator\n",
    "\n",
    "        metrics.update({\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision (Macro)': precision,\n",
    "            'Recall (Macro)': recall,\n",
    "            'F1-score (Macro)': f1,\n",
    "            'ROC AUC (Macro OVR)': roc_auc,\n",
    "            'Training Time': train_time\n",
    "        })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# --- individual model performance: only saves ROC plots ---\n",
    "def _save_individual_model_roc_plot(labels_test, scores, nClasses, writeDir, filename_identifier=\"\", target_labels=None):\n",
    "    \"\"\"\n",
    "    Saves individual model ROC plots to the specified directory.\n",
    "    Does NOT save any CSV files.\n",
    "\n",
    "    Args:\n",
    "        labels_test (np.array): True labels.\n",
    "        scores (np.array): Prediction scores (probabilities).\n",
    "        nClasses (int): Number of classes.\n",
    "        writeDir (str): The base directory where files will be saved.\n",
    "        filename_identifier (str, optional): An identifier to prepend to filenames.\n",
    "                                            Defaults to \"\".\n",
    "        target_labels (list, optional): List of class names for multiclass plotting.\n",
    "    \"\"\"\n",
    "    # Ensure the base directory exists before writing any files\n",
    "    os.makedirs(writeDir, exist_ok=True)\n",
    "\n",
    "    # Binary Classifier\n",
    "    if nClasses == 2:\n",
    "        fpr, tpr, _ = roc_curve(labels_test, scores[:, 1])\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "        roc_display.figure_.savefig(os.path.join(writeDir, f\"{filename_identifier}ROC.png\"))\n",
    "        plt.close(roc_display.figure_) # Close the figure to free memory\n",
    "\n",
    "    # Multiclass\n",
    "    elif nClasses > 2:\n",
    "        # Ensure target_labels are provided for multiclass ROC plotting\n",
    "        if target_labels is None:\n",
    "            print(\"Warning: target_labels not provided for multiclass ROC plotting. Using default names.\")\n",
    "            target_labels = [f\"Class_{i}\" for i in range(nClasses)]\n",
    "\n",
    "        for labelID in range(nClasses):\n",
    "            name = target_labels[labelID]\n",
    "            # roc_curve needs binary_truths and scores for that class\n",
    "            fpr, tpr, _ = roc_curve(labels_test[:,labelID], scores[:,labelID])\n",
    "            roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "            roc_display.figure_.savefig(os.path.join(writeDir, f\"{filename_identifier}ROC_{name}.png\"))\n",
    "            plt.close(roc_display.figure_) # Close the figure to free memory\n",
    "\n",
    "    print(f\"ROC plots saved with identifier '{filename_identifier}' in '{writeDir}'\")\n",
    "\n",
    "# --- Write Combined Ensemble Results to a Single CSV ---\n",
    "def write_combined_ensemble_results_csv(all_metrics_data, writeDir, nClasses):\n",
    "    \"\"\"\n",
    "    Writes combined performance metrics from multiple models (including ensemble)\n",
    "    into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        all_metrics_data (list): A list of dictionaries, where each dictionary\n",
    "                                 contains metrics for a model/ensemble.\n",
    "        writeDir (str): The directory where the combined CSV will be saved.\n",
    "        nClasses (int): Number of classes (to determine header).\n",
    "    \"\"\"\n",
    "    os.makedirs(writeDir, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "    combined_csv_path = os.path.join(writeDir, \"SWAFP_Combined_Ensemble_Results.csv\")\n",
    "\n",
    "    # Define the header based on nClasses\n",
    "    if nClasses == 2:\n",
    "        header = ['Model Name', 'Positive Labels', 'Negative Labels', 'TP', 'FP', 'TN', 'FN',\n",
    "                  'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Training Time']\n",
    "    elif nClasses > 2:\n",
    "        # The metrics dictionary from _calculate_metrics defines these for multiclass\n",
    "        header = ['Model Name', 'Accuracy', 'Precision (Macro)', 'Recall (Macro)',\n",
    "                  'F1-score (Macro)', 'ROC AUC (Macro OVR)', 'Training Time']\n",
    "    else:\n",
    "        print(f\"Warning: nClasses={nClasses} is not supported for combined CSV header.\")\n",
    "        return\n",
    "\n",
    "    with open(combined_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for metrics_dict in all_metrics_data:\n",
    "            writer.writerow(metrics_dict)\n",
    "    print(f\"Combined ensemble results saved to: {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "all_fold_results = []\n",
    "\n",
    "for i in range(num_folds):\n",
    "    print(f\"\\n===== FOLD {i+1}/{num_folds} =====\")\n",
    "\n",
    "### ===== DATA ORGANISATION ===== ###\n",
    "# Identify validation and training fold directories\n",
    "    val_dir = fold_dirs[i]\n",
    "    if i >=(num_folds -1): \n",
    "        test_dir = fold_dirs[0]\n",
    "    else:\n",
    "        test_dir = fold_dirs[i+1]\n",
    "    train_dirs = [fold_dirs[d] for d in range(num_folds) if fold_dirs[d] != (val_dir) and fold_dirs[d] != (test_dir)]\n",
    "    \n",
    "    print(f\"Validation fold: {val_dir.name}\")\n",
    "    print(f\"Test fold: {test_dir.name}\")\n",
    "    print(f\"Training folds: {[d.name for d in train_dirs]}\")\n",
    "\n",
    "# Load the validation dataset\n",
    "    val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "        class_names = target_labels, \n",
    "        # colour_mode = \"rgb\" -> three colour channels image\n",
    "        batch_size = batch_size,\n",
    "        image_size = (img_height, img_width),\n",
    "        shuffle = True, \n",
    "        seed = seed\n",
    "    )\n",
    "\n",
    "# Load the test dataset\n",
    "    test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "        class_names = target_labels, \n",
    "        # colour_mode = \"rgb\" -> three colour channels image\n",
    "        batch_size = batch_size,\n",
    "        image_size = (img_height, img_width),\n",
    "        shuffle = True, \n",
    "        seed = seed\n",
    "    )\n",
    "\n",
    "# Sixty Forty Split \n",
    "    val_dataset = val_dataset.concatenate(test_dataset) # combined testing and validation together\n",
    "\n",
    "# Load and combine the training datasets\n",
    "    train_datasets = []\n",
    "    for train_dir in train_dirs:\n",
    "        ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            train_dir,\n",
    "            label_mode='categorical', #(sparse -> image can only belong to one group)\n",
    "            class_names = target_labels, \n",
    "            # colour_mode = \"rgb\" -> three colour channels image\n",
    "            batch_size = batch_size,\n",
    "            image_size = (img_height, img_width),\n",
    "            shuffle = True, \n",
    "            seed = seed\n",
    "        )\n",
    "        train_datasets.append(ds)\n",
    "    \n",
    "# Combine all training datasets into one\n",
    "    train_dataset = train_datasets[0]\n",
    "    for ds in train_datasets[1:]:\n",
    "        train_dataset = train_dataset.concatenate(ds)\n",
    "\n",
    "# Set up Class Weights \n",
    "    # 1. Extract labels from the training dataset\n",
    "    train_labels = []\n",
    "    for _, labels in train_dataset:\n",
    "        train_labels.append(labels.numpy())\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "    train_labels = np.argmax(train_labels, axis=1)\n",
    "    # 2. Use scikit-learn's utility function to calculate weights\n",
    "    class_weights_array = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_labels),\n",
    "        y=train_labels\n",
    "    )\n",
    "    # 3. Create a dictionary for Keras\n",
    "    class_weights = dict(enumerate(class_weights_array))\n",
    "    print(f\"Class Weights for Fold {i+1}: {class_weights}\")\n",
    "\n",
    "    filepath = os.path.join(net_dir, f\"Fold{i+1}\")\n",
    "    os.mkdir(filepath)\n",
    "\n",
    "# ----- TRAINING & TESTING: USING SINGLE MODEL OR ENSEMBLE ENB3/AF/MI --------\n",
    "    split_layers = [125, 198, 272, 385]\n",
    "    results = []\n",
    "    graph_labels = [] # This will store labels for each plot line\n",
    "    \n",
    "    ### ====== TRAINING ===== ###\n",
    "    if Pretrained_model == \"ENB3\": \n",
    "        print(\"--- Starting Ensemble ENB3 Training ---\")\n",
    "        ENB3_model = keras.applications.EfficientNetB3(\n",
    "            include_top = True)\n",
    "\n",
    "        for layer in ENB3_model.layers[:385]:\n",
    "            layer.trainable = False # false: freezing, true: unfreeze \n",
    "        \n",
    "        for split_layer in split_layers:\n",
    "            cutModel = tf.keras.Model(ENB3_model.input, ENB3_model.layers[split_layer-1].output)\n",
    "            EnsembleENB3_model = add_classification_layers(cutModel)\n",
    "\n",
    "            modelpath = filepath + \"//\" + f\"{training_style}_{split_layer}\" + \"_\" + \"model.keras\"\n",
    "            print(f\"--- Training model truncated at layer {split_layer} --- \")\n",
    "            t_start_fold = time.time() # Start time for training\n",
    "            history = training_function(EnsembleENB3_model, modelpath, train_dataset, val_dataset)\n",
    "            results.append(history)\n",
    "            t_end_fold = time.time() # End time for training\n",
    "    \n",
    "        combined_filepath = os.path.join(net_dir, training_style)\n",
    "    \n",
    "    else:\n",
    "        print(f\"--- Starting Ensemble {Pretrained_model} Training ---\")\n",
    "        for idx, split_layer in enumerate(split_layers): \n",
    "            loaded_model = tf.keras.models.load_model(\n",
    "                    filepath = pretrained_net_dir + \"//\" + str(split_layer) + \".h5\")\n",
    "        \n",
    "            for layer in loaded_model.layers[:split_layer]:\n",
    "                layer.trainable = False # false: freezing, true: unfreeze \n",
    "            \n",
    "            cutModel = tf.keras.Model(loaded_model.input, loaded_model.layers[split_layer-1].output)\n",
    "            full_model = add_classification_layers(cutModel)\n",
    "            \n",
    "            modelpath = filepath + \"//\" + f\"{training_style}_{split_layer}\" + \"_\" + \"model.keras\"\n",
    "            # Run training and append the history object to the results list\n",
    "            print(f\"--- Training model truncated at layer {split_layer} --- \")\n",
    "            t_start_fold = time.time() # Start time for training\n",
    "            results.append(training_function(full_model, modelpath, train_dataset, val_dataset)) #softmax \n",
    "            t_end_fold = time.time() # End time for training\n",
    "        \n",
    "    ### ===== TESTING ===== ###\n",
    "    all_ensemble_metrics = [] # List to store metrics for all models and ensemble\n",
    "    ensemble_scores_list = [] # To collect scores for ensemble averaging\n",
    "\n",
    "# Assume true_labels and binary_true_labels are consistent across all batches for val_ds. Initialize outside the loop if val_ds is consistently processed\n",
    "    first_batch, first_label = next(iter(val_dataset)) # Get first batch to determine true_labels structure\n",
    "    true_labels_template = np.concatenate([np.array(l) for _, l in val_dataset]) # Get all true labels\n",
    "    binary_true_labels = np.argmax(true_labels_template, axis=1)\n",
    "\n",
    "    for idx, split_layer in enumerate(split_layers):\n",
    "        modelpath = filepath + \"//\" + f\"{training_style}_{split_layer}\" + \"_\" + \"model.keras\"\n",
    "        model = tf.keras.models.load_model(modelpath)\n",
    "        temp_score = []\n",
    "        for batch, label in val_dataset: # Re-iterate val_ds to get scores for current model\n",
    "            batch_score = model(batch)\n",
    "            temp_score.append (np.array(batch_score))\n",
    "        model_score = np.concatenate(temp_score)\n",
    "\n",
    "        # model_prediction is derived from model_score for this specific model\n",
    "        model_prediction = np.argmax(model_score, axis=1)\n",
    "\n",
    "        # Save individual ROC plot for this model\n",
    "        _save_individual_model_roc_plot(\n",
    "            binary_true_labels, model_score, nClasses,\n",
    "            writeDir= filepath, filename_identifier=f\"Model_{split_layer}_\",\n",
    "            target_labels=target_labels if 'target_labels' in globals() else None\n",
    "            )\n",
    "        print(f\"Ensemble Model {split_layer} ROC plot saved.\")\n",
    "\n",
    "        # Calculate metrics for this individual model and add to list\n",
    "        model_metrics = _calculate_metrics(\n",
    "            binary_true_labels, model_prediction, model_score, nClasses,\n",
    "            t_start = t_start_fold,\n",
    "            t_end = t_end_fold,\n",
    "            model_name=f\"Ensemble Model {split_layer}\"\n",
    "        )\n",
    "        all_ensemble_metrics.append(model_metrics)\n",
    "        ensemble_scores_list.append(model_score)\n",
    "\n",
    "    # Calculate ensemble average performance\n",
    "    ensemble_scores_array = np.array(ensemble_scores_list)\n",
    "    average_ensemble_score = np.mean(ensemble_scores_array, axis=0) # Corrected axis\n",
    "    ensemble_prediction = np.argmax(average_ensemble_score, axis=1)\n",
    "\n",
    "    # Save individual ROC plot for the ensemble average\n",
    "    _save_individual_model_roc_plot(\n",
    "        binary_true_labels, average_ensemble_score, nClasses,\n",
    "        writeDir= filepath, filename_identifier=\"Ensemble_Average_\",\n",
    "        target_labels=target_labels if 'target_labels' in globals() else None\n",
    "    )\n",
    "    print(\"Ensemble Average ROC plot saved.\")\n",
    "\n",
    "    # Calculate metrics for the ensemble average and add to list\n",
    "    ensemble_metrics = _calculate_metrics(\n",
    "        binary_true_labels, ensemble_prediction, average_ensemble_score, nClasses,\n",
    "        t_start=0, t_end=0, # Use 0 or a relevant ensemble inference time\n",
    "        model_name=\"Ensemble Average\"\n",
    "    )\n",
    "    all_ensemble_metrics.append(ensemble_metrics)\n",
    "\n",
    "    # Finally, write all collected metrics to a single combined CSV\n",
    "    write_combined_ensemble_results_csv(all_ensemble_metrics, filepath, nClasses)\n",
    "    print(\"Combined ensemble results CSV saved.\")\n",
    "\n",
    "    model_prediction = ensemble_prediction\n",
    "    model_score = average_ensemble_score\n",
    "\n",
    "### ===== Collect results for the current fold using the modified function ===== ###\n",
    "    fold_metrics = collect_fold_results(\n",
    "        labels_test = binary_true_labels,\n",
    "        ensemble_prediction = model_prediction,\n",
    "        ensemble_scores = model_score,\n",
    "        nClasses = nClasses,\n",
    "        t_start = t_start_fold, \n",
    "        t_end = t_end_fold,    \n",
    "        fold_number = i + 1,    # Pass the current fold number\n",
    "        output_base_dir = net_dir      # Pass the directory to save ROC plots\n",
    "    )\n",
    "    all_fold_results.append(fold_metrics)\n",
    "    print(f\"Metrics for Fold {i+1}: {fold_metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- After the loop: Consolidate and save all results to a single CSV ---\n",
    "print(\"\\n===== Consolidating Results =====\")\n",
    "\n",
    "results_df = pd.DataFrame(all_fold_results)\n",
    "\n",
    "# --- DEBUGGING STEP: Print the DataFrame and its columns immediately after creation ---\n",
    "print(\"\\n--- Debugging results_df content after creation ---\")\n",
    "print(\"Is results_df empty?\", results_df.empty)\n",
    "if not results_df.empty:\n",
    "    print(\"results_df columns:\", results_df.columns.tolist())\n",
    "    print(\"First 3 rows of results_df (if available):\\n\", results_df.head(3))\n",
    "else:\n",
    "    print(\"results_df is empty. No results were collected from folds.\")\n",
    "    print(\"Please check your cross-validation loop and 'collect_fold_results' function.\")\n",
    "    # If the DataFrame is empty, there's nothing to average, so we can exit or skip\n",
    "    exit()\n",
    "\n",
    "# Add an \"Average\" row for overall performance\n",
    "if 'Fold' in results_df.columns:\n",
    "    cols_to_average = [col for col in results_df.columns if col != 'Fold']\n",
    "else:\n",
    "    print(\"\\nWARNING: 'Fold' column not found in DataFrame. Attempting to average all numeric columns.\")\n",
    "    cols_to_average = results_df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "if cols_to_average:\n",
    "    average_row = results_df[cols_to_average].mean().to_dict()\n",
    "    average_row['Fold'] = 'Average'\n",
    "    for col in ['Positive Labels', 'Negative Labels', 'TP', 'FP', 'TN', 'FN']:\n",
    "        if col in average_row:\n",
    "            average_row[col] = int(round(average_row[col]))\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([average_row])], ignore_index=True)\n",
    "else:\n",
    "    print(\"\\nWARNING: No numeric columns found in results_df to calculate an average.\")\n",
    "\n",
    "# Define the path for the final CSV file\n",
    "final_output_csv_path = f\"{net_dir}/5_Folds_Cross_Validation_Summary.csv\"\n",
    "\n",
    "# Save the DataFrame to a single CSV file\n",
    "results_df.to_csv(final_output_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nAll {num_folds} fold results and overall average saved to: {final_output_csv_path}\")\n",
    "print(\"\\nFinal Results DataFrame:\")\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if training_style == \"SWOIP\":\n",
    "    #     SWOIP_model = keras.applications.EfficientNetB3(\n",
    "    #         include_top = True)\n",
    "    #     for layer in SWOIP_model.layers[:385]: #frozen to layer 125, unfreezed the rest \n",
    "    #         layer.trainable = False \n",
    "    #     ### ====== TRAINING ===== ###\n",
    "    #     split_layer = 385\n",
    "    #     cutModel = tf.keras.Model(SWOIP_model.input, SWOIP_model.layers[split_layer-1].output)\n",
    "    #     SWOIP_model = add_classification_layers(cutModel)\n",
    "    #     modelpath = filepath + \"//\" + training_style + \"_\" + \"model.keras\"\n",
    "    #     results = training_function(SWOIP_model, modelpath, train_dataset, val_dataset)\n",
    "    #     t_start_fold = time.time() # Start time for training\n",
    "    #     results = training_function(full_model)\n",
    "    #     t_end_fold = time.time() # End time for training\n",
    "    #     ### ===== TESTING ===== ### \n",
    "    #     temp_score = []\n",
    "    #     temp_labels = []\n",
    "    #     for batch, label in val_dataset: # test_dataset \n",
    "    #         batch_score = full_model(batch)\n",
    "    #         temp_score.append (np.array(batch_score))\n",
    "    #         temp_labels.append (np.array (label))\n",
    "    #     model_score = np.concatenate(temp_score)\n",
    "    #     true_labels = np.concatenate(temp_labels)\n",
    "    #     model_prediction = np.argmax(model_score, axis=1)\n",
    "    #     binary_true_labels = np.argmax (true_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = results[1].history['val_loss']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results[1].history['loss'], label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract validation loss for easier access\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# ### 1. Find the Best Epoch and Minimum Validation Loss\n",
    "# # Use np.argmin to find the INDEX of the minimum loss\n",
    "# best_epoch_idx = np.argmin(val_loss)\n",
    "# min_val_loss = val_loss[best_epoch_idx]\n",
    "\n",
    "# print(f\"Minimum validation loss of {min_val_loss:.4f} was found at epoch {best_epoch_idx}.\")\n",
    "\n",
    "# ### 2. Calculate a Suggested Patience ###\n",
    "# # Define how much worse the loss can get before we consider it overfitting\n",
    "# tolerance_percentage = 0.05 # e.g., 5% worse than the minimum\n",
    "# overfit_threshold = min_val_loss * (1 + tolerance_percentage)\n",
    "\n",
    "# # Find the first epoch *after* the best epoch where loss exceeds the threshold\n",
    "# patience_epoch = -1\n",
    "# for i in range(best_epoch_idx + 1, len(val_loss)):\n",
    "#     if val_loss[i] > overfit_threshold:\n",
    "#         patience_epoch = i\n",
    "#         break # Stop as soon as we find it\n",
    "\n",
    "# # The suggested patience is the number of epochs between the best and the overfit point\n",
    "# suggested_patience = -1\n",
    "# if patience_epoch != -1:\n",
    "#     suggested_patience = patience_epoch - best_epoch_idx\n",
    "\n",
    "# ### 3. Plot the Results and Annotations ###\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "\n",
    "# # Annotate the minimum validation loss\n",
    "# plt.axvline(x = best_epoch_idx, color = 'r', linestyle='--', label = f'Best Epoch: {best_epoch_idx}')\n",
    "# plt.scatter(best_epoch_idx, min_val_loss, color = 'red', zorder = 5) # Mark the best point\n",
    "\n",
    "# # Annotate the suggested patience point, if found\n",
    "# if suggested_patience > 0:\n",
    "#     plt.axvline(x=patience_epoch, color='g', linestyle='--', label=f'Recommended patience: {suggested_patience}')\n",
    "#     plt.scatter(patience_epoch, val_loss[patience_epoch], color='green', zorder=5)\n",
    "#     print(f\"A good starting patience value could be around {suggested_patience}.\")\n",
    "#     print(f\"This is how many epochs it took for the validation loss to increase by more than {tolerance_percentage:.0%} from its minimum (at {patience_epoch} epoch).\")\n",
    "# else:\n",
    "#     print(\"\\nThe model did not appear to overfit within the training run (based on the defined tolerance).\")\n",
    "\n",
    "# plt.title('Training and Validation Loss Analysis')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss (Categorical Cross Entropy)') \n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeltraining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
